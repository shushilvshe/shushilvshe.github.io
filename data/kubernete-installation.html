
<!DOCTYPE html>
<html lang="zh_cn">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="http://www.shushilvshe.com/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="http://www.shushilvshe.com/theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="http://www.shushilvshe.com/theme/font-awesome/css/font-awesome.min.css">




    <link rel="shortcut icon" href="http://www.shushilvshe.com/images/lvcheng.jpg" type="image/x-icon">
    <link rel="icon" href="http://www.shushilvshe.com/images/lvcheng.jpg" type="image/x-icon">



<meta name="author" content="ssls" />
<meta name="description" content="此次部署kubernetes1.8.3版本使用两台机器进行操作，一台做为Master节点，一台作为Node节点。部署流程及配置与正式环境下是一致的。" />
<meta name="keywords" content="大数据, 机器学习, 决策树">

<meta property="og:site_name" content="shushilvshe's Blog"/>
<meta property="og:title" content="kubernetes（1.8.3）系列之安装"/>
<meta property="og:description" content="此次部署kubernetes1.8.3版本使用两台机器进行操作，一台做为Master节点，一台作为Node节点。部署流程及配置与正式环境下是一致的。"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="http://www.shushilvshe.com/data/kubernete-installation.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-01-23 20:06:33+08:00"/>
<meta property="article:modified_time" content="2018-01-23 20:06:35+08:00"/>
<meta property="article:author" content="http://www.shushilvshe.com/author/ssls.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="大数据"/>
<meta property="article:tag" content="机器学习"/>
<meta property="article:tag" content="决策树"/>
<meta property="og:image" content="http://www.shushilvshe.com/images/self.jpg">

  <title>shushilvshe's Blog &ndash; kubernetes（1.8.3）系列之安装</title>

</head>
<body>

  <aside>
    <div>
      <a href="http://www.shushilvshe.com">
        <img src="http://www.shushilvshe.com/images/self.jpg" alt="数食旅摄" title="数食旅摄">
      </a>
      <h1><a href="http://www.shushilvshe.com">数食旅摄</a></h1>

<p>Data Developer</p>
      <nav>
        <ul class="list">
          <li><a href="http://www.shushilvshe.com/pages/about.html#about">About ME</a></li>

          <li><a href="/categories.html" target="_blank">Category</a></li>
          <li><a href="/tags.html" target="_blank">Tags</a></li>
          <li><a href="/archives.html" target="_blank">Archives</a></li>
        </ul>
      </nav>

      <ul class="social">
      </ul>
    </div>

  </aside>
  <main>

    <nav>
      <a href="http://www.shushilvshe.com">    Home
</a>

      <a href="/category/data.html">Data</a>
      <a href="/category/food.html">Food</a>
      <a href="/category/travel.html">Travel</a>
      <a href="/category/qixi/index.html">Qixi</a>


    </nav>

<article class="single">
  <header>
      
    <h1 id="data/kubernete-installation">kubernetes（1.8.3）系列之安装</h1>
    <p>
          Posted on 2018-01-23(星期二) 20:06 in <a href="http://www.shushilvshe.com/category/data.html">Data</a>


    </p>
  </header>


  <div>
    <p><link rel="stylesheet" href="http://yandex.st/highlightjs/6.2/styles/googlecode.min.css"></p>
<script src="http://code.jquery.com/jquery-1.7.2.min.js"></script>

<script src="http://yandex.st/highlightjs/6.2/highlight.min.js"></script>

<script>hljs.initHighlightingOnLoad();</script>

<script type="text/javascript">
 $(document).ready(function(){
      $("h2,h3,h4,h5,h6").each(function(i,item){
        var tag = $(item).get(0).localName;
        $(item).attr("id","wow"+i);
        $("#category").append('<a class="new'+tag+'" href="#wow'+i+'">'+$(this).text()+'</a></br>');
        $(".newh2").css("margin-left",0);
        $(".newh3").css("margin-left",30);
        $(".newh4").css("margin-left",60);
        $(".newh5").css("margin-left",90);
        $(".newh6").css("margin-left",120);
      });
 });
</script>

<div id="category"></div>

<h2>部署环境</h2>
<p>此次部署<code>kubernetes1.8.3</code>版本使用两台机器进行操作，一台做为<code>Master</code>节点，一台作为<code>Node</code>节点。部署流程及配置与正式环境下是一致的。</p>
<p><strong>Kubernetes Master节点：10.0.11.222</strong></p>
<p><strong>Kubernetes Node节点：10.0.11.221</strong></p>
<h2>部署软件</h2>
<ul>
<li>
<p><strong>CentOS：CentOS 7.3</strong></p>
</li>
<li>
<p><strong>Docker：17.03.2-ce</strong></p>
</li>
<li>
<p><strong>Kubernetes：1.8.3</strong></p>
</li>
<li>
<p><strong>etcd：3.2.7</strong></p>
</li>
<li>
<p><strong>flannel：v0.7.1</strong></p>
</li>
</ul>
<h2>准备工作</h2>
<p><code>master</code>节点、<code>node</code>节点都要做这些准备工作。</p>
<h3>关闭防火墙</h3>
<div class="highlight"><pre><span></span># 关闭防火墙
[root@mimo222 ~]# systemctl stop firewalld

# 禁用防火墙
[root@mimo222 ~]# systemctl disable firewalld
</pre></div>


<h3>禁用selinux</h3>
<div class="highlight"><pre><span></span># 设置selinux为关闭
[root@mimo222 ~]# setenforce 0

# 打开selinux配置文件
[root@mimo222 ~]# vim /etc/selinux/config

# SELINUX配置项修改为disabled
SELINUX=disabled
</pre></div>


<h2>创建验证</h2>
<p>此次安装<code>kubernetes1.8.3</code>，我们使用基于<code>CA签名</code>的数字证书认证方式，通过<code>cfssl</code>进行证书生成。此次安装是<code>单Master节点</code>，因此证书的生成都在<code>Master</code>节点完成。如果你安装的是<code>多Master节点</code>，那么可在其中一个<code>Master</code>节点生成证书，然后同步将证书拷贝到其他的<code>Master</code>（注意：多个<code>Master</code>节点的证书目录一定要一致，以避免不必要的问题出现）。最后，在配置<code>Node</code>节点时，我们可以从<code>Master</code>节点拷贝<code>Node</code>节点需要的证书。</p>
<h3>安装 cfssl</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# mkdir -p /opt/local/cfssl
[root@mimo222 ~]# cd /opt/local/cfssl
[root@mimo222 ~]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
[root@mimo222 ~]# mv cfssl_linux-amd64 cfssl
[root@mimo222 ~]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
[root@mimo222 ~]# mv cfssljson_linux-amd64 cfssljson
[root@mimo222 ~]# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
[root@mimo222 ~]# mv cfssl-certinfo_linux-amd64 cfssl-certinfo
[root@mimo222 ~]# chmod +x *
</pre></div>


<p>安装<code>cfssl</code>需要下载上面命令中的三个文件，如果通过<code>wget</code>无法下载，可通过直接在网页上访问该链接下载，然后拷贝到<code>/opt/local/cfssl</code>目录。<strong>注意修改文件权限。</strong></p>
<h3>创建 CA 证书配置</h3>
<p>创建<code>ssl</code>目录：</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# mkdir /opt/ssl
[root@mimo222 ~]# cd /opt/ssl
</pre></div>


<p>在<code>/opt/ssl</code>目录下创建<code>config.json</code>文件：</p>
<div class="highlight"><pre><span></span>vi  config.json

{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      }
    }
  }
}
</pre></div>


<p>在<code>/opt/ssl</code>目录下创建<code>csr.json</code> 文件：</p>
<p>vi csr.json</p>
<div class="highlight"><pre><span></span>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;TianJin&quot;,
      &quot;L&quot;: &quot;TianJin&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</pre></div>


<h3>生成 CA 证书和私钥</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /opt/ssl/

# 生成CA证书和私钥
[root@mimo222 ~]# /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca

[root@mimo222 ~]# ls -lt
total 5
-rw-r--r--  1 root root 1005 Dec 14 17:48 ca.csr
-rw-------  1 root root 1679 Dec 14 17:48 ca-key.pem
-rw-r--r--  1 root root 1363 Dec 14 17:48 ca.pem
-rw-r--r--. 1 root root  292 Dec 14 17:45 config.json
-rw-r--r--  1 root root  210 Dec 14 17:48 csr.json
</pre></div>


<h3>分发证书</h3>
<p>我们将所有<code>kubernetes</code>相关的证书都保存到<code>/etc/kubernetes/ssl</code>目录，方便管理。</p>
<div class="highlight"><pre><span></span># 创建证书目录
[root@mimo222 ~]# mkdir -p /etc/kubernetes/ssl

# 拷贝所有文件到目录下
[root@mimo222 ~]# cp *.pem /etc/kubernetes/ssl
[root@mimo222 ~]# cp ca.csr /etc/kubernetes/ssl

# 这里要将文件拷贝到所有的k8s Master机器上
# 这里演示一下多Master节点时证书的保存方式，在每次生成证书之后，都可以通过下面的方式将证书拷贝
# 到远程Master节点，保证数据的同步。下面其他证书方式一样，就不再写了。
[root@mimo222 ~]# scp *.pem xx.xx.xx.xx:/etc/kubernetes/ssl/
[root@mimo222 ~]# scp *.csr xx.xx.xx.xx:/etc/kubernetes/ssl/
</pre></div>


<h2>安装Docker</h2>
<p>所有<code>kubernetes</code>节点都需要安装<code>docker</code>，并且版本最好一致。</p>
<h3>开始安装</h3>
<div class="highlight"><pre><span></span># 删除老版本Docker及依赖，如果机器上装过其他版本的docker，可通过该命令进行老版本docker清理
[root@mimo222 ~]# yum remove docker docker-common container-selinux docker-selinux docker-engine

# 安装 yum-config-manager
[root@mimo222 ~]# yum -y install yum-utils

# 导入yum源
[root@mimo222 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

# 更新repo
[root@mimo222 ~]# yum makecache

# 查看yum版本
[root@mimo222 ~]# yum list docker-ce.x86_64  --showduplicates |sort -r

# 安装指定版本docker-ce 17.03被docker-ce-selinux依赖,不能直接yum安装docker-ce-selinux
# 如果此处通过wget无法下载，可到网上查一下这个rpm包，下载下来然后拷贝到服务器上。

[root@mimo222 ~]# wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm

[root@mimo222 ~]# rpm -ivh docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm

# 安装docker
[root@mimo222 ~]# yum -y install docker-ce-17.03.2.ce
</pre></div>


<h3>查看docker版本</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# docker version

Client:
 Version:      17.03.2-ce
 API version:  1.27
 Go version:   go1.7.5
 Git commit:   f5ec1e2
 Built:        Tue Jun 27 02:21:36 2017
 OS/Arch:      linux/amd64

Server:
 Version:      17.03.2-ce
 API version:  1.27 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   f5ec1e2
 Built:        Tue Jun 27 02:21:36 2017
 OS/Arch:      linux/amd64
 Experimental: false
</pre></div>


<h3>更改docker配置</h3>
<p>更改<code>/usr/lib/systemd/system/docker.service</code>启动文件</p>
<div class="highlight"><pre><span></span><span class="x">[root@mimo222 ~]</span><span class="err">#</span><span class="x"> vi /usr/lib/systemd/system/docker.service</span>

<span class="x">[Unit]</span>
<span class="x">Description=Docker Application Container Engine</span>
<span class="x">Documentation=https://docs.docker.com</span>
<span class="x">After=network-online.target firewalld.service</span>
<span class="x">Wants=network-online.target</span>

<span class="x">[Service]</span>
<span class="x">Type=notify</span>

<span class="x">ExecStart=/usr/bin/dockerd </span><span class="p">$</span><span class="nv">DOCKER_NETWORK_OPTIONS</span><span class="x"> </span><span class="p">$</span><span class="nv">DOCKER_OPTS</span><span class="x"> </span><span class="p">$</span><span class="nv">DOCKER_DNS_OPTIONS</span><span class="x"></span>

<span class="err">#</span><span class="x"> 添加这行操作，在每次重启docker之前都会设置iptables策略为ACCEPT</span>
<span class="x">ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT</span>

<span class="x">ExecReload=/bin/kill -s HUP </span><span class="p">$</span><span class="nv">MAINPID</span><span class="x"></span>

<span class="x">LimitNOFILE=infinity</span>
<span class="x">LimitNPROC=infinity</span>
<span class="x">LimitCORE=infinity</span>
<span class="x">TimeoutStartSec=0</span>
<span class="x">Delegate=yes</span>
<span class="x">KillMode=process</span>
<span class="x">Restart=on-failure</span>
<span class="x">StartLimitBurst=3</span>
<span class="x">StartLimitInterval=60s</span>

<span class="x">[Install]</span>
<span class="x">WantedBy=multi-user.target</span>
</pre></div>


<p>对比初始安装<code>docker</code>后的<code>docker.service</code>文件内容，修改<code>docker.service</code>文件。</p>
<p><code>Docker</code>从<code>1.13</code>版本开始，可能将<code>iptables FORWARD chain</code>的默认策略设置为<code>DROP</code>，从而导致<code>ping</code>其他<code>Node</code>上的<code>Pod IP</code>失败。因此，需要在docker.service文件中添加<code>ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT</code>。</p>
<h3>修改其他配置（Docker启动参数配置）</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# mkdir -p /usr/lib/systemd/system/docker.service.d/
</pre></div>


<p><code>docker-options.conf</code>该文件中保存<code>docker</code>启动的一些参数</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# vi /usr/lib/systemd/system/docker.service.d/docker-options.conf

# 文件中添加信息如下：(注意 Environment配置信息必须在同一行，如果出现换行会无法加载)

[Service]
Environment=&quot;DOCKER_OPTS=--insecure-registry=10.254.0.0/16 --graph=/opt/docker --disable-legacy-registry --insecure-registry 10.0.11.222:5000 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock&quot;
</pre></div>


<p><code>--insecure-registry 10.0.11.222:5000</code>：这个是我本地私有镜像库地址，大家根据自己需要修改。</p>
<p><code>-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock</code>：<code>Swarm</code>集群需要的agent配置。</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# vi /usr/lib/systemd/system/docker.service.d/docker-dns.conf

# 添加如下 :

[Service]
Environment=&quot;DOCKER_DNS_OPTIONS=\
    --dns 10.254.0.2 --dns 114.114.114.114  \
    --dns-search default.svc.cluster.local --dns-search svc.cluster.local  \
    --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2&quot;
</pre></div>


<p><code>flannel.conf</code>：该文件保存了读取<code>flannel</code>网络分段信息</p>
<div class="highlight"><pre><span></span># 1. 该配置文件用于和flannel网络交互，若使用的网络配置不是flannel，则无需该配置文件
# 2. 该配置文件需在flannel安装配置之后才能生效

vi /usr/lib/systemd/system/docker.service.d/flannel.conf

添加如下信息：

[Service]
EnvironmentFile=-/run/flannel/docker
</pre></div>


<p>启动、重启、查看<code>docker</code></p>
<div class="highlight"><pre><span></span># 重新读取配置
[root@mimo222 ~]# systemctl daemon-reload

# 启动docker
[root@mimo222 ~]# systemctl start docker

# 设置为开机启动
[root@mimo222 ~]# systemctl enable docker

# 查看docker运行状态
[root@mimo222 ~]# systemctl status docker

# 重启docker
[root@mimo222 ~]# systemctl restart docker

# 停止运行docker
[root@mimo222 ~]# systemctl stop docker
</pre></div>


<h2>etcd安装配置</h2>
<p><code>etcd</code>是<code>kubernetes</code>集群最重要的组件， <code>etcd</code>挂了，集群就挂了。</p>
<h3>安装etcd</h3>
<p>这里我是在别的地方下载了<code>etcd-3.2.9-3.el7.x86_64.rpm</code>，然后拷贝到服务器上进行安装。</p>
<div class="highlight"><pre><span></span>rpm -ivh etcd-3.2.9-3.el7.x86_64.rpm
</pre></div>


<h3>创建etcd证书</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /opt/ssl/

[root@mimo222 ~]# vi etcd-csr.json

# 添加如下内容：

{
  &quot;CN&quot;: &quot;etcd&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;10.0.11.222&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;TianJin&quot;,
      &quot;L&quot;: &quot;TianJin&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</pre></div>


<p>其中，<code>hosts</code>中填写的是etcd节点的地址，这里只安装了单节点<code>etcd</code>，因此只填写了<code>etcd</code>本地地址<code>127.0.0.1</code>以及<code>10.0.11.222</code>。如果安装的是<code>etcd</code>集群，则需要在<code>hosts</code>中添加别的<code>etcd</code>节点的<code>ip</code>地址。</p>
<div class="highlight"><pre><span></span># 生成 etcd 密钥

/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
  -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/config.json \
  -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd
</pre></div>


<p><code>/opt/local/cfssl/cfssl</code>：使用cfssl进行证书生成</p>
<div class="highlight"><pre><span></span># 查看生成信息

[root@mimo222 ~]# ll etcd*

-rw-r--r-- 1 root root 1050 Dec 14 18:31 etcd.csr
-rw-r--r-- 1 root root  257 Dec 14 18:31 etcd-csr.json
-rw------- 1 root root 1679 Dec 14 18:31 etcd-key.pem
-rw-r--r-- 1 root root 1424 Dec 14 18:31 etcd.pem

# 拷贝
[root@mimo222 ~]# cp etcd*.pem /etc/kubernetes/ssl/

# 如果 etcd 非 root 用户，读取证书会提示没权限
[root@mimo222 ~]# chmod 644 /etc/kubernetes/ssl/etcd-key.pem
</pre></div>


<h3>修改etcd配置</h3>
<p>修改 <code>etcd</code> 配置文件<code>/etc/etcd/etcd.conf</code></p>
<div class="highlight"><pre><span></span><span class="cp"># 备份原来的etcd.conf配置文件</span>
<span class="n">mv</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">etcd</span><span class="o">/</span><span class="n">etcd</span><span class="p">.</span><span class="n">conf</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">etcd</span><span class="o">/</span><span class="n">etcd</span><span class="p">.</span><span class="n">conf</span><span class="o">-</span><span class="n">bak</span>

<span class="cp"># 重新写一份配置文件</span>
<span class="n">vi</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">etcd</span><span class="o">/</span><span class="n">etcd</span><span class="p">.</span><span class="n">conf</span>

<span class="cp"># [member]</span>
<span class="n">ETCD_NAME</span><span class="o">=</span><span class="n">etcd1</span>  <span class="err">#</span> <span class="n">etcd节点名称</span>
<span class="n">ETCD_DATA_DIR</span><span class="o">=</span><span class="s">&quot;/var/lib/etcd/etcd1.etcd&quot;</span>
<span class="n">ETCD_WAL_DIR</span><span class="o">=</span><span class="s">&quot;/var/lib/etcd/wal&quot;</span>
<span class="n">ETCD_SNAPSHOT_COUNT</span><span class="o">=</span><span class="s">&quot;100&quot;</span>
<span class="n">ETCD_HEARTBEAT_INTERVAL</span><span class="o">=</span><span class="s">&quot;100&quot;</span>
<span class="n">ETCD_ELECTION_TIMEOUT</span><span class="o">=</span><span class="s">&quot;1000&quot;</span>
<span class="n">ETCD_LISTEN_PEER_URLS</span><span class="o">=</span><span class="s">&quot;https://[Node IP]:2380&quot;</span> 
<span class="n">ETCD_LISTEN_CLIENT_URLS</span><span class="o">=</span><span class="s">&quot;https://0.0.0.0:2379&quot;</span>
<span class="n">ETCD_MAX_SNAPSHOTS</span><span class="o">=</span><span class="s">&quot;5&quot;</span>
<span class="n">ETCD_MAX_WALS</span><span class="o">=</span><span class="s">&quot;5&quot;</span>
<span class="cp">#ETCD_CORS=&quot;&quot;</span>

<span class="cp"># [cluster]</span>
<span class="n">ETCD_INITIAL_ADVERTISE_PEER_URLS</span><span class="o">=</span><span class="s">&quot;https://[Node IP]:2380&quot;</span>
<span class="cp"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http:</span><span class="c1">//...&quot;</span>
<span class="n">ETCD_INITIAL_CLUSTER</span><span class="o">=</span><span class="s">&quot;etcd1=https://[Node IP]:2380&quot;</span>
<span class="n">ETCD_INITIAL_CLUSTER_STATE</span><span class="o">=</span><span class="s">&quot;new&quot;</span>
<span class="n">ETCD_INITIAL_CLUSTER_TOKEN</span><span class="o">=</span><span class="s">&quot;k8s-etcd-cluster&quot;</span>
<span class="n">ETCD_ADVERTISE_CLIENT_URLS</span><span class="o">=</span><span class="s">&quot;https://[Node IP]:2379&quot;</span>
<span class="cp">#ETCD_DISCOVERY=&quot;&quot;</span>
<span class="cp">#ETCD_DISCOVERY_SRV=&quot;&quot;</span>
<span class="cp">#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;</span>
<span class="cp">#ETCD_DISCOVERY_PROXY=&quot;&quot;</span>
<span class="cp">#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;</span>
<span class="cp">#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;</span>

<span class="cp"># [proxy]</span>
<span class="cp">#ETCD_PROXY=&quot;off&quot;</span>
<span class="cp">#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;</span>
<span class="cp">#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;</span>
<span class="cp">#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;</span>
<span class="cp">#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;</span>
<span class="cp">#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;</span>

<span class="cp"># [security]</span>
<span class="n">ETCD_CERT_FILE</span><span class="o">=</span><span class="s">&quot;/etc/kubernetes/ssl/etcd.pem&quot;</span>
<span class="n">ETCD_KEY_FILE</span><span class="o">=</span><span class="s">&quot;/etc/kubernetes/ssl/etcd-key.pem&quot;</span>
<span class="n">ETCD_CLIENT_CERT_AUTH</span><span class="o">=</span><span class="s">&quot;true&quot;</span>
<span class="n">ETCD_TRUSTED_CA_FILE</span><span class="o">=</span><span class="s">&quot;/etc/kubernetes/ssl/ca.pem&quot;</span>
<span class="n">ETCD_AUTO_TLS</span><span class="o">=</span><span class="s">&quot;true&quot;</span>
<span class="n">ETCD_PEER_CERT_FILE</span><span class="o">=</span><span class="s">&quot;/etc/kubernetes/ssl/etcd.pem&quot;</span>
<span class="n">ETCD_PEER_KEY_FILE</span><span class="o">=</span><span class="s">&quot;/etc/kubernetes/ssl/etcd-key.pem&quot;</span>
<span class="n">ETCD_PEER_CLIENT_CERT_AUTH</span><span class="o">=</span><span class="s">&quot;true&quot;</span>
<span class="n">ETCD_PEER_TRUSTED_CA_FILE</span><span class="o">=</span><span class="s">&quot;/etc/kubernetes/ssl/ca.pem&quot;</span>
<span class="n">ETCD_PEER_AUTO_TLS</span><span class="o">=</span><span class="s">&quot;true&quot;</span>

<span class="cp"># [logging]</span>
<span class="cp">#ETCD_DEBUG=&quot;false&quot;</span>
<span class="cp"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span>
<span class="cp">#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot;</span>
</pre></div>


<p>注意：
1、将上面的配置文件中的<code>[Node IP]</code>都修改为本机IP，这里我安装在10.0.11.222，因此都修改成了10.0.11.222。 </p>
<p>2、<code>[security]</code>下的配置项指定了证书的位置，因此要注意证书的位置是否正确。</p>
<p>3、如果安装的是etcd集群，配置文件信息基本一致，只要修改<code>[Node IP]</code>为etcd节点的主机IP即可，同时修改<code>ETCD_INITIAL_CLUSTER</code>项，该项需要添加所有etcd节点，例如：
<code>ETCD_INITIAL_CLUSTER="etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380"</code>。</p>
<h3>启动 etcd</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# systemctl enable etcd

[root@mimo222 ~]# systemctl start etcd

[root@mimo222 ~]# systemctl status etcd

# 如果报错 请使用
journalctl -f -t etcd  和 journalctl -u etcd 来定位问题
[root@mimo222 ~]# journalctl -f -t etcd
</pre></div>


<h3>验证 etcd 状态</h3>
<h4>查看 etcd 状态</h4>
<p>若为<code>etcd</code>集群，则<code>--endpoints</code>需要填写所有<code>etcd</code>节点<code>IP:PORT</code></p>
<div class="highlight"><pre><span></span>etcdctl --endpoints=https://10.0.11.222:2379\
        --cert-file=/etc/kubernetes/ssl/etcd.pem \
        --ca-file=/etc/kubernetes/ssl/ca.pem \
        --key-file=/etc/kubernetes/ssl/etcd-key.pem \
        cluster-health

member 2012db49e3efb509 is healthy: got healthy result from https://10.0.11.222:2379
cluster is healthy
</pre></div>


<h4>查看 etcd 集群成员</h4>
<div class="highlight"><pre><span></span>etcdctl --endpoints=https://10.0.11.222:2379\
        --cert-file=/etc/kubernetes/ssl/etcd.pem \
        --ca-file=/etc/kubernetes/ssl/ca.pem \
        --key-file=/etc/kubernetes/ssl/etcd-key.pem \
        member list

2012db49e3efb509: name=etcd1 peerURLs=http://10.0.11.222:2380 clientURLs=https://10.0.11.222:2379 isLeader=true
</pre></div>


<h2>Kubernetes Master节点安装配置</h2>
<p><code>Master</code>需要部署 <code>kube-apiserver</code> , <code>kube-scheduler</code> , <code>kube-controller-manager</code> 这三个组件。 <code>kube-scheduler</code> 作用是调度<code>pods</code>分配到那个<code>node</code>里，简单来说就是资源调度。 <code>kube-controller-manager</code> 作用是 对 <code>deployment controller</code> , <code>replication controller</code>, <code>endpoints controller</code>, <code>namespace controller</code> and <code>serviceaccounts controller</code>等等的循环控制，与<code>kube-apiserver</code>交互。</p>
<h3>安装组件</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# mkdir /tmp/kubernetes

[root@mimo222 ~]# cd /tmp/kubernetes

# 这个包比较大，几百兆，我通过wget没有下下来... 直接在网页上输入这个网址，然后一点一点下载下来了..
# 大家可以用别的方法下载，然后拷贝到服务器上即可。

[root@mimo222 ~]# wget https://dl.k8s.io/v1.8.3/kubernetes-server-linux-amd64.tar.gz

[root@mimo222 ~]# tar -xzvf kubernetes-server-linux-amd64.tar.gz

[root@mimo222 ~]# cd kubernetes

# Master节点不需要kubelet、kube-proxy，因此这里没有拷贝这俩，如果要将master节点也装成node节点，
# 则kubelet、kube-proxy这俩就需要拷贝出来了。

[root@mimo222 ~]# cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl} /usr/local/bin/
</pre></div>


<h3>创建admin证书</h3>
<p><code>kubectl</code> 与 <code>kube-apiserver</code> 的安全端口通信，需要为安全通信提供 <code>TLS</code> 证书和秘钥。</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /opt/ssl/

[root@mimo222 ~]# vi admin-csr.json

# 添加以下信息:

{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;TianJin&quot;,
      &quot;L&quot;: &quot;TianJin&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</pre></div>


<p>生成<code>admin</code> 证书和私钥</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /opt/ssl/

/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/opt/ssl/config.json \
  -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin
</pre></div>


<p>查看生成</p>
<div class="highlight"><pre><span></span>[root@mimo222 ssl]# ll admin*

-rw-r--r-- 1 root root 1013 Dec 14 19:22 admin.csr
-rw-r--r-- 1 root root  231 Dec 14 19:22 admin-csr.json
-rw------- 1 root root 1679 Dec 14 19:22 admin-key.pem
-rw-r--r-- 1 root root 1407 Dec 14 19:22 admin.pem
</pre></div>


<p>拷贝到指定目录</p>
<div class="highlight"><pre><span></span>cp admin*.pem /etc/kubernetes/ssl/
</pre></div>


<h3>配置 kubectl kubeconfig 文件</h3>
<p>生成证书相关的配置文件存储与 <code>/root/.kube</code> 目录中</p>
<div class="highlight"><pre><span></span># 配置 kubernetes 集群

kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443


# 配置 客户端认证

kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/ssl/admin-key.pem


kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin


kubectl config use-context kubernetes
</pre></div>


<h3>创建 kubernetes 证书</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /opt/ssl

[root@mimo222 ~]# vi kubernetes-csr.json

{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;10.0.11.222&quot;,
    &quot;10.254.0.1&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;TianJin&quot;,
      &quot;L&quot;: &quot;TianJin&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</pre></div>


<p>这里 <code>hosts</code> 字段中三个 IP 分别为 <code>127.0.0.1</code> 本机、<code>172.16.1.64</code> 为 <code>Master</code> 的<code>IP</code>（多个<code>Master</code>节点需要将所有<code>master</code>节点<code>IP</code>都写在这）、 <code>10.254.0.1</code> 为 <code>kubernetes SVC</code>的 <code>IP</code>， 一般是部署网络的第一个<code>IP</code> , 如: <code>10.254.0.1</code>， 在启动完成后，我们使用  <code>kubectl get svc</code>， 就可以查看到。</p>
<h3>生成 kubernetes 证书和私钥</h3>
<div class="highlight"><pre><span></span>/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/opt/ssl/config.json \
  -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes
</pre></div>


<p>查看生成</p>
<div class="highlight"><pre><span></span>[root@kube_master222 ssl]# ll kubernetes*

-rw-r--r-- 1 root root 1245 12月 19 14:34 kubernetes.csr
-rw-r--r-- 1 root root  437 12月 19 14:25 kubernetes-csr.json
-rw------- 1 root root 1675 12月 19 14:34 kubernetes-key.pem
-rw-r--r-- 1 root root 1610 12月 19 14:34 kubernetes.pem
</pre></div>


<p>拷贝到指定目录</p>
<div class="highlight"><pre><span></span>cp kubernetes*.pem /etc/kubernetes/ssl/
</pre></div>


<h3>配置 kube-apiserver</h3>
<p><code>kubelet</code> 首次启动时向<code>kube-apiserver</code> 发送 <code>TLS Bootstrapping</code> 请求，<code>kube-apiserver</code> 验证 <code>kubelet</code> 请求中的 <code>token</code> 是否与它配置的 <code>token</code> 一致，如果一致则自动为 <code>kubelet</code>生成证书和秘钥。</p>
<div class="highlight"><pre><span></span># 生成 token（注意此处的token在生成node节点kubelet需要的证书时也会用到，要注意不要写错或重新生成别的）
[root@mimo222 ~]# head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;
3140a1541451afcc87ca7b715f124ce3

# 创建 token.csv 文件
[root@mimo222 ~]# cd /opt/ssl
[root@mimo222 ~]# vi token.csv

# 填写以下信息：
3140a1541451afcc87ca7b715f124ce3,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;

# 拷贝
cp token.csv /etc/kubernetes/

# 生成高级审核配置文件
[root@mimo222 ~]# cd /etc/kubernetes

cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF
# Log all requests at the Metadata level.
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
- level: Metadata
EOF
</pre></div>


<h3>创建 kube-apiserver.service 文件</h3>
<ul>
<li>自定义 系统 service 文件一般存于 <code>/etc/systemd/system/</code> 下</li>
<li>
<p>配置为 各自的本地<code>IP</code></p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# vi /etc/systemd/system/kube-apiserver.service

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \
  --advertise-address=10.0.11.222 \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/log/kubernetes/audit.log \
  --authorization-mode=Node,RBAC \
  --anonymous-auth=false \ # 不接受匿名访问，若为true，则表示接受，此处设置为false，便于dashboard访问
  --bind-address=0.0.0.0 \
  --secure-port=6443 \
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \
  --enable-swagger-ui=true \
  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \
  --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \
  --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \
  --etcd-servers=https://10.0.11.222:2379 \
  --event-ttl=1h \
  --kubelet-https=true \
  --insecure-bind-address=127.0.0.1 \
  --insecure-port=8080 \
  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --service-cluster-ip-range=10.254.0.0/16 \
  --service-node-port-range=30000-32000 \
  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --enable-bootstrap-token-auth \
  --token-auth-file=/etc/kubernetes/token.csv \
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</pre></div>


</li>
<li>
<p><code>--advertise-address</code>：master节点本机IP</p>
</li>
<li>
<p><code>--etcd-servers：etcd URL</code>：etcd集群需要写上所有etcd节点 URL</p>
</li>
<li>
<p><code>--service-node-port-range=30000-32000</code>：这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。</p>
</li>
<li>
<p>注意验证证书等路径是否正确。</p>
</li>
</ul>
<h3>启动 kube-apiserver</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# systemctl daemon-reload
[root@mimo222 ~]# systemctl enable kube-apiserver
[root@mimo222 ~]# systemctl start kube-apiserver
[root@mimo222 ~]# systemctl status kube-apiserver
</pre></div>


<h3>配置 kube-controller-manager</h3>
<p>创建 <code>kube-controller-manager.service</code>文件</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# vi /etc/systemd/system/kube-controller-manager.service


[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
  --address=0.0.0.0 \
  --master=http://127.0.0.1:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.254.0.0/16 \
  --cluster-cidr=10.233.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \
  --leader-elect=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</pre></div>


<h3>启动 kube-controller-manager</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# systemctl daemon-reload
[root@mimo222 ~]# systemctl enable kube-controller-manager
[root@mimo222 ~]# systemctl start kube-controller-manager
[root@mimo222 ~]# systemctl status kube-controller-manager
</pre></div>


<h3>配置 kube-scheduler</h3>
<p>创建 <code>kube-cheduler.service</code> 文件</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# vi /etc/systemd/system/kube-scheduler.service


[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --address=0.0.0.0 \
  --master=http://127.0.0.1:8080 \
  --leader-elect=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</pre></div>


<h3>启动 kube-scheduler</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# systemctl daemon-reload
[root@mimo222 ~]# systemctl enable kube-scheduler
[root@mimo222 ~]# systemctl start kube-scheduler
[root@mimo222 ~]# systemctl status kube-scheduler
</pre></div>


<h3>验证 Master 节点</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ssl]# kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}


# 由于还没有加入node节点，所有查询node时为空。

[root@mimo222 ssl]# kubectl get nodes
NAME          STATUS    ROLES     AGE       VERSION
</pre></div>


<h2>flannel安装配置</h2>
<p><code>kubernetes</code>要求集群内各节点能通过<code>Pod</code>网段互联互通，本节介绍使用<code>Flannel</code>在所有节点 <code>(Master、Node)</code> 上创建互联互通的 <code>Pod</code> 网段的步骤。</p>
<h3>创建flannel证书</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /opt/ssl/

[root@mimo222 ~]# vi flanneld-csr.json

# 添加如下内容：

{
  &quot;CN&quot;: &quot;flanneld&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;TianJin&quot;,
      &quot;L&quot;: &quot;TianJin&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</pre></div>


<p><strong>注意：hosts 字段为空。</strong></p>
<h3>生成 flanneld 证书和私钥</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /opt/ssl

[root@mimo222 ~]# /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
    -ca-key=/opt/ssl/ca-key.pem \
    -config=/opt/ssl/config.json \
    -profile=kubernetes flanneld-csr.json | /opt/local/cfssl/cfssljson -bare flanneld

# 查看生成信息

[root@mimo222 ~]# ll flannel*

-rw-r--r-- 1 root root 1001 Dec 15 13:59 flanneld.csr
-rw-r--r-- 1 root root  223 Dec 15 13:56 flanneld-csr.json
-rw------- 1 root root 1679 Dec 15 13:59 flanneld-key.pem
-rw-r--r-- 1 root root 1395 Dec 15 13:59 flanneld.pem

# 拷贝
[root@mimo222 ~]# cp flanneld*.pem /etc/kubernetes/ssl/
</pre></div>


<h3>向etcd写入集群Pod网段信息</h3>
<p><strong>注意：本步骤只需在第一次部署 Flannel 网络时执行，后续在其它节点上部署 Flannel 时无需再写入该信息！</strong></p>
<div class="highlight"><pre><span></span>etcdctl --endpoints=https://10.0.11.222:2379 \
--ca-file=/opt/ssl/ca.pem \
--cert-file=/etc/kubernetes/ssl/flanneld.pem \
--key-file=/etc/kubernetes/ssl/flanneld-key.pem \
set /kubernetes/network/config &#39;{&quot;Network&quot;:&quot;10.233.0.0/16&quot;}&#39;
</pre></div>


<p>写入的 <code>Pod</code> 网段(<code>{"Network":"10.233.0.0/16"}</code>) 必须与 <code>kube-controller-manager</code> 的 <code>--cluster-cidr</code> 选项值一致；</p>
<h3>安装和配置flanneld</h3>
<h4>下载 flanneld</h4>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# mkdir /tmp/flannel
[root@mimo222 ~]# cd /tmp/flannel
[root@mimo222 ~]# wget https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz
[root@mimo222 ~]# tar -xzvf flannel-v0.7.1-linux-amd64.tar.gz
[root@mimo222 ~]# cp {flanneld,mk-docker-opts.sh} /usr/local/bin
</pre></div>


<h4>创建启动文件flanneld.service</h4>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# vi /etc/systemd/system/flanneld.service

# 添加以下信息:

[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/usr/local/bin/flanneld \
  -etcd-cafile=/etc/kubernetes/ssl/ca.pem \
  -etcd-certfile=/etc/kubernetes/ssl/flanneld.pem \
  -etcd-keyfile=/etc/kubernetes/ssl/flanneld-key.pem \
  -etcd-endpoints=https://10.0.11.222:2379 \
  -etcd-prefix=/kubernetes/network
ExecStartPost=/usr/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
</pre></div>


<ol>
<li>
<p><code>-etcd-endpoints</code> ：填写<code>etcd</code>配置的<code>URL</code>，注意使用<code>https</code>。</p>
</li>
<li>
<p><code>-etcd-prefix</code>：上面第一步设置的<code>Pod</code>网段<code>key</code>前缀。我设置的<code>key</code>为<code>/kubernetes/network/config</code>，因此前缀为<code>/kubernetes/network</code>。</p>
</li>
<li>
<p>注意CA验证证书的路径是否正确。</p>
</li>
</ol>
<h3>启动 flanneld</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# systemctl daemon-reload 
[root@mimo222 ~]# systemctl enable flanneld
[root@mimo222 ~]# systemctl start flanneld
[root@mimo222 ~]# systemctl status flanneld
</pre></div>


<h3>etcd中flannel网段信息查询</h3>
<p>对于以下命令，<code>--endpoints</code>参数后跟的是<code>etcd</code>集群<code>IP:PORT</code>（多个<code>etcd</code>节点需填多个<code>IP:PORT</code>），<code>/kubernetes/network</code>是我这里设置的<code>FLANNEL_ETCD_PREFIX</code>，因此大家可根据自己设置的<code>flannel etcd 前缀</code>来填写。<code>--ca-file</code>、<code>--cert-file</code>、<code>--key-file</code>是指相应flannel证书路径，若没有采用CA验证，则不需要。</p>
<p>下面的命令仅是我这里用的一些命令例子，其中参数需要根据自己需要修改一下。</p>
<h4>查看ETCD中flannel集群 Pod 网段</h4>
<div class="highlight"><pre><span></span>etcdctl --endpoints=https://10.0.11.222:2379 \
    --ca-file=/opt/ssl/ca.pem \
    --cert-file=/etc/kubernetes/ssl/flanneld.pem \
    --key-file=/etc/kubernetes/ssl/flanneld-key.pem \
get /kubernetes/network/config

{&quot;Network&quot;:&quot;10.233.0.0/16&quot;}
</pre></div>


<h4>查看已分配的 Pod 子网段列表</h4>
<div class="highlight"><pre><span></span>etcdctl --endpoints=https://10.0.11.222:2379 \
    --ca-file=/opt/ssl/ca.pem \
    --cert-file=/etc/kubernetes/ssl/flanneld.pem \
    --key-file=/etc/kubernetes/ssl/flanneld-key.pem \
ls /kubernetes/network/subnets

/kubernetes/network/subnets/10.233.50.0-24
/kubernetes/network/subnets/10.233.86.0-24
</pre></div>


<h4>查看某一 Pod 网段对应的 flanneld 进程监听的 IP 和网络参数</h4>
<div class="highlight"><pre><span></span>etcdctl --endpoints=https://10.0.11.222:2379 \
    --ca-file=/opt/ssl/ca.pem \
    --cert-file=/etc/kubernetes/ssl/flanneld.pem \
    --key-file=/etc/kubernetes/ssl/flanneld-key.pem \
get /kubernetes/network/subnets/10.233.50.0-24

{&quot;PublicIP&quot;:&quot;10.0.11.221&quot;}
</pre></div>


<h4>确保各节点间 Pod 网段能互联互通</h4>
<p>在各节点上部署完 Flannel 后，根据上面命令查看已分配的 Pod 子网段列表（此处是我分配的网段列表）：</p>
<div class="highlight"><pre><span></span>/kubernetes/network/subnets/10.233.50.0-24
/kubernetes/network/subnets/10.233.86.0-24
</pre></div>


<p>在各节点上分配 ping 这两个网段的网关地址，确保能通：</p>
<div class="highlight"><pre><span></span>ping 10.233.50.1
ping 10.233.86.1
</pre></div>


<h2>Kubernetes Node节点安装配置</h2>
<p><code>kubernetes node</code>节点需要安装<code>kubelet</code>、<code>kube-proxy</code>、<code>flannel</code>。<strong>下面的配置步骤，指明在master节点执行的需要在master节点执行，没有指明的在node节点执行。</strong></p>
<h3>配置 kubelet（Master节点执行）</h3>
<p><code>kubelet</code> 启动时向 <code>kube-apiserver</code> 发送 <code>TLS bootstrapping</code> 请求，需要先将 <code>bootstrap token</code>文件中的 <code>kubelet-bootstrap</code> 用户赋予 <code>system:node-bootstrapper</code> 角色，然后 <code>kubelet</code>才有权限创建认证请求<code>(certificatesigningrequests)</code>。</p>
<div class="highlight"><pre><span></span># 先创建认证请求
# user 为 master 中 token.csv 文件里配置的用户
# 只需创建一次就可以

kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
</pre></div>


<h3>创建 kubelet kubeconfig 文件（Master节点执行）</h3>
<div class="highlight"><pre><span></span># 配置集群（server要写master ip地址，不要使用127.0.0.1或localhost，否则node节点无法识别）

kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://10.0.11.222:6443 \
  --kubeconfig=bootstrap.kubeconfig

# 配置客户端认证（此处的token使用token.csv中的token）

kubectl config set-credentials kubelet-bootstrap \
  --token=3140a1541451afcc87ca7b715f124ce3 \
  --kubeconfig=bootstrap.kubeconfig


# 配置关联

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig


# 配置默认关联
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig

# 拷贝生成的 bootstrap.kubeconfig 文件

mv bootstrap.kubeconfig /etc/kubernetes/
</pre></div>


<h3>配置 kube-proxy（Master节点执行）</h3>
<h4>创建 kube-proxy 证书（Master节点执行）</h4>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /opt/ssl
[root@mimo222 ~]# vi kube-proxy-csr.json

{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;TianJin&quot;,
      &quot;L&quot;: &quot;TianJin&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</pre></div>


<h4>生成 kube-proxy 证书和私钥（Master节点执行）</h4>
<div class="highlight"><pre><span></span>/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/opt/ssl/config.json \
  -profile=kubernetes  kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy

# 查看生成
[root@mimo222 ssl]# ll kube-proxy*
-rw-r--r-- 1 root root 1013 Dec 14 20:09 kube-proxy.csr
-rw-r--r-- 1 root root  232 Dec 14 20:09 kube-proxy-csr.json
-rw------- 1 root root 1679 Dec 14 20:09 kube-proxy-key.pem
-rw-r--r-- 1 root root 1407 Dec 14 20:09 kube-proxy.pem

# 拷贝到目录
cp kube-proxy*.pem /etc/kubernetes/ssl/
</pre></div>


<h4>创建 kube-proxy kubeconfig 文件（Master节点执行）</h4>
<div class="highlight"><pre><span></span># 配置集群（server要写master ip地址，不要使用127.0.0.1或localhost，否则node节点无法识别）
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://10.0.11.222:6443 \
  --kubeconfig=kube-proxy.kubeconfig

# 配置客户端认证（注意证书路径）
kubectl config set-credentials kube-proxy \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

# 配置关联
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

# 配置默认关联
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

# 拷贝到目录
mv kube-proxy.kubeconfig /etc/kubernetes/
</pre></div>


<h3>从master节点拷贝证书</h3>
<p>所有需要的证书生成最好都在master节点进行生成，然后统一管理，<code>node</code>节点需要哪些，直接从<code>master</code>拷贝即可。</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# mkdir -p /etc/kubernetes/ssl/
[root@mimo222 ~]# cd /etc/kubernetes/

# kubelet kubeconfig 文件
[root@mimo222 ~]# scp 10.0.11.222:/etc/kubernetes/bootstrap.kubeconfig ./

# kube-proxy kubeconfig 文件
[root@mimo222 ~]# scp 10.0.11.222:/etc/kubernetes/kube-proxy.kubeconfig ./
[root@mimo222 ~]# cd /etc/kubernetes/ssl

# kubernetes相关证书
[root@mimo222 ~]# scp 10.0.11.222:/etc/kubernetes/ssl/ca.pem ./
[root@mimo222 ~]# scp 10.0.11.222:/etc/kubernetes/ssl/kube-proxy.pem ./
[root@mimo222 ~]# scp 10.0.11.222:/etc/kubernetes/ssl/kube-proxy-key.pem ./

# flannel证书
[root@mimo222 ~]# scp 10.0.11.222:/etc/kubernetes/ssl/flanneld-key.pem ./
[root@mimo222 ~]# scp 10.0.11.222:/etc/kubernetes/ssl/flanneld.pem ./
</pre></div>


<h3>node节点flannel配置</h3>
<h4>下载 flanneld</h4>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# mkdir /tmp/flannel
[root@mimo222 ~]# cd /tmp/flannel
[root@mimo222 ~]# wget https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz
[root@mimo222 ~]# tar -xzvf flannel-v0.7.1-linux-amd64.tar.gz
[root@mimo222 ~]# cp {flanneld,mk-docker-opts.sh} /usr/local/bin
</pre></div>


<h4>配置并启动flanneld</h4>
<p>创建启动文件<code>flanneld.service</code>：</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# vi /etc/systemd/system/flanneld.service

# 添加以下信息:

[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/usr/local/bin/flanneld \
  -etcd-cafile=/etc/kubernetes/ssl/ca.pem \
  -etcd-certfile=/etc/kubernetes/ssl/flanneld.pem \
  -etcd-keyfile=/etc/kubernetes/ssl/flanneld-key.pem \
  -etcd-endpoints=https://10.0.11.222:2379 \
  -etcd-prefix=/kubernetes/network
ExecStartPost=/usr/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
</pre></div>


<ol>
<li>
<p><code>-etcd-endpoints</code> ：填写<code>etcd</code>配置的<code>URL</code>，注意使用<code>https</code>。</p>
</li>
<li>
<p><code>-etcd-prefix</code>：上面第一步设置的<code>Pod</code>网段key前缀。我设置的<code>key</code>为<code>/kubernetes/network/config</code>，因此前缀为<code>/kubernetes/network</code>。</p>
</li>
<li>
<p>注意<code>CA</code>验证证书的路径是否正确。</p>
</li>
</ol>
<p>启动 <code>flanneld</code>：</p>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# systemctl daemon-reload 
[root@mimo222 ~]# systemctl enable flanneld
[root@mimo222 ~]# systemctl start flanneld
[root@mimo222 ~]# systemctl status flanneld
</pre></div>


<h3>安装kubelet、kube-proxy</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# cd /tmp/kubernetes

# master节点安装时使用过这个文件，拷到node节点即可
[root@mimo222 ~]# wget https://dl.k8s.io/v1.8.3/kubernetes-server-linux-amd64.tar.gz
[root@mimo222 ~]# tar -xzvf kubernetes-server-linux-amd64.tar.gz
[root@mimo222 ~]# cd kubernetes
[root@mimo222 ~]# cp -r server/bin/{kube-proxy,kubelet} /usr/local/bin/
</pre></div>


<h3>配置 kubelet.service 文件</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# mkdir /var/lib/kubelet
[root@mimo222 ~]# vi /etc/systemd/system/kubelet.service

[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
  --cgroup-driver=cgroupfs \
  --hostname-override=10.0.11.221 \
  --pod-infra-container-image=jicki/pause-amd64:3.0 \
  --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --cert-dir=/etc/kubernetes/ssl \
  --cluster_dns=10.254.0.2 \
  --cluster_domain=cluster.local. \
  --hairpin-mode promiscuous-bridge \
  --allow-privileged=true \
  --fail-swap-on=false \
  --serialize-image-pulls=false \
  --logtostderr=true \
  --max-pods=512 \
  --v=2

[Install]
WantedBy=multi-user.target
</pre></div>


<ol>
<li>
<p><code>--pod-infra-container-image</code>：该项配置的是pod启动时需要一块启动的<code>pod-infrastructure</code>镜像，真实使用时最好将该文件下载到本地私有库，然后将该项配置成私有库的<code>pod-infrastructure</code>镜像下载地址。</p>
</li>
<li>
<p><code>--kubeconfig</code>： <code>kubelet</code>启动成功之后自动生成的<code>kubelet.kubeconfig</code>文件保存路径。</p>
</li>
</ol>
<h3>启动 kubelet</h3>
<div class="highlight"><pre><span></span>[root@mimo222 ~]# systemctl daemon-reload
[root@mimo222 ~]# systemctl enable kubelet
[root@mimo222 ~]# systemctl start kubelet
[root@mimo222 ~]# systemctl status kubelet

# 如果报错 请使用
journalctl -f -t kubelet  和 journalctl -u kubelet 来定位问题
</pre></div>


<h3>配置 TLS 认证（Master节点执行）</h3>
<p>此步骤很重要，当<code>node</code>节点安装好<code>kubelet</code>之后，首次启动，会请求<code>master</code>节点的<code>apiserver</code>，此时会生成一个<code>csr</code>，可根据<code>kubectl get csr</code>查看。只有<code>master</code>节点认证了该<code>csr</code>之后，<code>node</code>节点才能够真正与<code>master</code>建立通信，完成认证。否则<code>node</code>节点与<code>master</code>节点无法交互。</p>
<div class="highlight"><pre><span></span># 查看 csr 的名称

[root@mimo222 ~]# kubectl get csr

NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-A9WKNWyqyq89XOwg-uqCu2C4fBQEhOhzmlQJ6f8VPWE   22h       kubelet-bootstrap   Approved,Issued
node-csr-J1W94p6S2w0fjTkvpdfG0J-lRY-dmkVFH01OG3R6T4Y   3h        kubelet-bootstrap   Approved,Issued
node-csr-aVIfz5k6GX5jy31z43ZkhxzzLfkFtMUoAkJTi1Okcx8   3h        kubelet-bootstrap   Pending

# 若CONDITION显示为Pending，可增加认证
# 下面的语句是将所有Pending状态的csr设置已认证
kubectl get csr | grep Pending | awk &#39;{print $1}&#39; | xargs kubectl certificate approve
</pre></div>


<h3>验证 nodes（Master节点执行）</h3>
<div class="highlight"><pre><span></span>kubectl get nodes

NAME          STATUS    ROLES     AGE       VERSION
10.0.11.221   Ready     &lt;none&gt;    3h        v1.8.3
</pre></div>


<h3>node节点验证</h3>
<p><code>kubelet</code>启动成功之后，会请求<code>apiserver</code>，通过验证，<code>node</code>节点本地机器会自动生成配置文件与密钥，查看<code>/etc/kubernetes</code></p>
<div class="highlight"><pre><span></span>[root@mimo221 kubernetes]# ll

-rw-------. 1 root root 2195 Dec 15 10:27 bootstrap.kubeconfig
-rw-------. 1 root root 2286 Dec 15 15:43 kubelet.kubeconfig
-rw-------. 1 root root 6305 Dec 15 10:28 kube-proxy.kubeconfig
drwxr-xr-x. 2 root root  202 Dec 15 15:43 ssl

[root@mimo221 kubernetes]# ll ssl |grep kubelet

-rw-r--r--. 1 root root 1046 Dec 15 15:43 kubelet-client.crt
-rw-------. 1 root root  227 Dec 15 15:38 kubelet-client.key
-rw-r--r--. 1 root root 1111 Dec 15 15:38 kubelet.crt
-rw-------. 1 root root 1675 Dec 15 15:38 kubelet.key
</pre></div>


<h3>配置kube-proxy.service 文件</h3>
<div class="highlight"><pre><span></span>[root@mimo221 kubernetes]# mkdir -p /var/lib/kube-proxy

[root@mimo221 kubernetes]# vi /etc/systemd/system/kube-proxy.service

[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
  --bind-address=0.0.0.0 \
  --hostname-override=10.0.11.221 \
  --cluster-cidr=10.254.0.0/16 \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
  --logtostderr=true \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</pre></div>


<p><code>--hostname-override</code>：要于<code>kubelet</code>配置文件中的<code>--hostname-override</code>值一致。</p>
<h3>启动 kube-proxy</h3>
<div class="highlight"><pre><span></span>[root@mimo221 kubernetes]# systemctl daemon-reload
[root@mimo221 kubernetes]# systemctl enable kube-proxy
[root@mimo221 kubernetes]# systemctl start kube-proxy
[root@mimo221 kubernetes]# systemctl status kube-proxy

# 如果报错 请使用
journalctl -f -t kube-proxy  和 journalctl -u kube-proxy 来定位问题
</pre></div>


<p>到此如果都没有报错的话，说明<code>kubernetes1.8.3</code>集群安装成功了。</p>
<h2>问题集锦</h2>
<h3>1.关于nvidia设备</h3>
<blockquote>
<p>安装完nvidia驱动和cuda驱动后，会在系统中创建三个设备（如果有多块儿显卡，则会多出来nvidia1、nvidia2等设备），但重启的时候，这三个设备默认没有加载。</p>
</blockquote>
<div class="highlight"><pre><span></span>[root@gpu189 /]# ll /dev | grep nvidia
crw-rw-rw- 1 root root    195,   0 1月  19 15:59 nvidia0
crw-rw-rw- 1 root root    195, 255 1月  19 15:59 nvidiactl
crw-rw-rw- 1 root root    244,   0 1月  19 15:59 nvidia-uvm
</pre></div>


<p>那么我们可以通过以下脚本启动加载。 创建 <code>/root/bin/check-nvidia.sh</code> 脚本。</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

/sbin/modprobe nvidia

<span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;</span><span class="nv">$?</span><span class="s2">&quot;</span> -eq <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="c1"># Count the number of NVIDIA controllers found.</span>
  <span class="nv">NVDEVS</span><span class="o">=</span><span class="sb">`</span>lspci <span class="p">|</span> grep -i NVIDIA<span class="sb">`</span>
  <span class="nv">N3D</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span> <span class="s2">&quot;</span><span class="nv">$NVDEVS</span><span class="s2">&quot;</span> <span class="p">|</span> grep <span class="s2">&quot;3D controller&quot;</span> <span class="p">|</span> wc -l<span class="sb">`</span>
  <span class="nv">NVGA</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span> <span class="s2">&quot;</span><span class="nv">$NVDEVS</span><span class="s2">&quot;</span> <span class="p">|</span> grep <span class="s2">&quot;VGA compatible controller&quot;</span> <span class="p">|</span> wc -l<span class="sb">`</span>

  <span class="nv">N</span><span class="o">=</span><span class="sb">`</span>expr <span class="nv">$N3D</span> + <span class="nv">$NVGA</span> - 1<span class="sb">`</span>
  <span class="k">for</span> i in <span class="sb">`</span>seq <span class="m">0</span> <span class="nv">$N</span><span class="sb">`</span><span class="p">;</span> <span class="k">do</span>
    mknod -m <span class="m">666</span> /dev/nvidia<span class="nv">$i</span> c <span class="m">195</span> <span class="nv">$i</span>
  <span class="k">done</span>

  mknod -m <span class="m">666</span> /dev/nvidiactl c <span class="m">195</span> 255

<span class="k">else</span>
  <span class="nb">exit</span> 1
<span class="k">fi</span>

/sbin/modprobe nvidia-uvm

<span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;</span><span class="nv">$?</span><span class="s2">&quot;</span> -eq <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="c1"># Find out the major device number used by the nvidia-uvm driver</span>
  <span class="nv">D</span><span class="o">=</span><span class="sb">`</span>grep nvidia-uvm /proc/devices <span class="p">|</span> awk <span class="s1">&#39;{print $1}&#39;</span><span class="sb">`</span>

  mknod -m <span class="m">666</span> /dev/nvidia-uvm c <span class="nv">$D</span> 0
<span class="k">else</span>
  <span class="nb">exit</span> 1
<span class="k">fi</span>
</pre></div>
</td></tr></table>

<p>创建服务 <code>check-nvidia-dev.service</code></p>
<div class="highlight"><pre><span></span><span class="k">[Unit]</span>
<span class="na">Description</span><span class="o">=</span><span class="s">check nvidia dev</span>
<span class="na">After</span><span class="o">=</span><span class="s">network-online.target firewalld.service</span>

<span class="k">[Service]</span>
<span class="na">ExecStart</span><span class="o">=</span><span class="s">/root/bin/check-nvidia.sh</span>

<span class="k">[Install]</span>
<span class="na">WantedBy</span><span class="o">=</span><span class="s">multi-user.target</span>
</pre></div>


<p>设置成开机启动</p>
<div class="highlight"><pre><span></span>[root@gpu188 system]# systemctl enable check-nvidia-dev.service 
Created symlink from /etc/systemd/system/multi-user.target.wants/check-nvidia-dev.service to /usr/lib/systemd/system/check-nvidia-dev.service.
</pre></div>


<p>在docker服务中添加 <strong>check-nvidia-dev.service</strong> 依赖</p>
<div class="highlight"><pre><span></span><span class="k">[Unit]</span>
<span class="na">Description</span><span class="o">=</span><span class="s">Docker Application Container Engine</span>
<span class="na">Documentation</span><span class="o">=</span><span class="s">https://docs.docker.com</span>
<span class="na">After</span><span class="o">=</span><span class="s">network-online.target firewalld.service check-nvidia-dev.service</span>
<span class="na">Wants</span><span class="o">=</span><span class="s">network-online.target</span>
</pre></div>


<p>最后更新一下服务即可。</p>
<div class="highlight"><pre><span></span>systemctl daemon-reload
</pre></div>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="http://www.shushilvshe.com/tag/da-shu-ju.html">大数据</a>
      <a href="http://www.shushilvshe.com/tag/ji-qi-xue-xi.html">机器学习</a>
      <a href="http://www.shushilvshe.com/tag/jue-ce-shu.html">决策树</a>
    </p>
  </div>



    <div class="addthis_relatedposts_inline">


</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " shushilvshe's Blog ",
  "url" : "http://www.shushilvshe.com",
  "image": "http://www.shushilvshe.com/images/self.jpg",
  "description": "shushilvshe's Thoughts and Writings"
}
</script>

</body>
</html>