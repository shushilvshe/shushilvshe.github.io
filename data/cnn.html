<!DOCTYPE html>
<html lang="en">
<head>
        <title>shushilvshe's Blog : 卷积神经网络（CNN）</title>
        <meta charset="utf-8" />
        <link rel="stylesheet" href="http://www.shushilvshe.com/theme/css/main.css" type="text/css" />
        <link href="http://www.shushilvshe.com/" type="application/atom+xml" rel="alternate" title="shushilvshe's Blog ATOM Feed" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://www.shushilvshe.com/css/ie.css"/>
                <script src="http://www.shushilvshe.com/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://www.shushilvshe.com/css/ie6.css"/><![endif]-->

</head>

<body>
        
<header>
    <h1><a href="http://www.shushilvshe.com/data/cnn.html" id="page-title">卷积神经网络（CNN）</a></h1>
    <span id="sitename"><a href="http://www.shushilvshe.com" id="site-title">shushilvshe's Blog  <strong>Data Developer</strong></a> &sdot;</span>
<time datetime="2017-09-13T13:59:15+08:00">2017-09-13(星期三) 13:59</time></header>
<article>
    <h3>概述</h3>
<p><strong>卷积神经网络(Convolutional Neural Network, CNN)</strong>是深度学习技术中极具代表的网络结构之一，在图像处理领域取得了很大的成功，在国际标准的ImageNet数据集上，许多成功的模型都是基于CNN的。CNN相较于传统的图像处理算法的优点之一在于，避免了对图像复杂的前期预处理过程（提取人工特征等），可以直接输入原始图像。</p>
<p>图像处理中，往往会将图像看成是一个或多个的二维向量，如之前博文中提到的MNIST手写体图片就可以看做是一个28 × 28的二维向量（黑白图片，只有一个颜色通道；如果是RGB表示的彩色图片则有三个颜色通道，可表示为三张二维向量）。传统的神经网络都是采用全连接的方式，即输入层到隐藏层的神经元都是全部连接的，这样做将导致参数量巨大，使得网络训练耗时甚至难以训练，而CNN则通过局部连接、权值共享等方法避免这一困难，有趣的是，这些方法都是受到现代生物神经网络相关研究的启发（感兴趣可阅读以下部分）。</p>
<p><center><img src="http://i.imgur.com/UK1cQOP.png" width="650"/></center></p>
<hr>
<p>卷积神经网络(CNN)的出现是为了解决MLP多层感知器全连接和梯度发散的问题，其核心思想：<strong>1.局部感知(local field)，2.权值共享(Shared Weights)，3.下采样(池化)(subsampling)。</strong>极大地提升了计算速度，减少了连接数量。</p>
<ul>
<li>
<p><strong>局部连接与权值共享</strong></p>
<p>下图是一个很经典的图示，左边是全连接，右边是局部连接。</p>
<p><center><img src="http://i.imgur.com/PHbta3D.jpg" width="650"/></center></p>
<p>对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。</p>
<p>尽管减少了几个数量级，但参数数量依然较多。能不能再进一步减少呢？能！方法就是<strong>权值共享</strong>。具体做法是，在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，<strong>将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同</strong>，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 <strong>10 × 10个权值参数</strong>（也就是卷积核(也称滤波器)的大小），如下图。</p>
<p><center><img src="http://i.imgur.com/IIBM59H.jpg" width="650"/></center></p>
<p>这大概就是CNN的一个神奇之处，尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为Feature Map。如果有100个卷积核，最终的权值参数也仅为100 × 100 = 10^4个而已。另外，偏置参数也是共享的，同一种滤波器共享一个。</p>
<p><strong>下面的动图能够更好地解释卷积过程：</strong></p>
<p><center><img src="http://i.imgur.com/KPyqPOB.gif" width="650"/></center></p>
</li>
<li>
<p><strong>池化</strong></p>
<p>在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。</p>
<p>pooling的好处：
　　
- 1. 这些统计特征能够有更低的维度，减少计算量。
- 2. 不容易过拟合，当参数过多的时候很容易造成过度拟合。
- 3. 缩小图像的规模，提升计算速度。</p>
<p>如下图所示，原图是一张500∗500 的图像，经过subsampling之后哦，变成了一张 250∗250 的图像。这样操作的好处非常明显，虽然经过权值共享和局部连接后的图像权值参数已经大大减少，但是对于计算量来说，还是非常巨大，需要消费很大的计算时间，于是为了进一步减少计算量，于是加入了subsampling这个概念，不仅仅使图像像素减少了， 同时也减少计算时间。</p>
<p><center><img src="http://www.openhw.org/data/image/f/5ef/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E5%9B%BE%E8%A7%A3_11.png?_=6502116" width="650"/></center></p>
<p>池化规模一般为2×2。常用的池化方法有：</p>
<ul>
<li>最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。</li>
<li>均值池化（Mean Pooling）。取4个点的均值。</li>
<li>可训练池化。训练函数 f ，接受4个点为输入，出入1个点。</li>
</ul>
<p>由于特征图的变长不一定是2的倍数，所以在边缘处理上也有两种方案：</p>
<ul>
<li><strong>保留边缘。</strong>将特征图的变长用0填充为2的倍数，然后再池化。</li>
<li><strong>忽略边缘。</strong>将多出来的边缘直接省去。</li>
</ul>
<p><center><img src="http://i.imgur.com/bHBUsr4.png" width="800"/></center></p>
</li>
</ul>
<blockquote>
<p>经典网络——<strong>LeNet-5网络详解</strong></p>
</blockquote>
<p><strong>经典的LeNet-5 网络图</strong></p>
<p><center><img src="http://img.blog.csdn.net/20150715101531291?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" width="800"/></center></p>
<ul>
<li><strong>C1层是一个卷积层，由6个特征图Feature Map构成。特征图中每个神经元与输入为5*5的邻域相连。特征图的大小为28*28，这样能防止输入的连接掉到边界之外（32-5+1=28）。C1有156个可训练参数（每个滤波器5*5=25个unit参数和一个bias参数，一共6个滤波器，共(5*5+1)*6=156个参数），共156*(28*28)=122,304个连接。</strong></li>
</ul>
<p><center><img src="http://i.imgur.com/2AuotA0.png" width="800"/></center></p>
<ul>
<li><strong>S2层是一个下采样层，有6个14*14的特征图。特征图中的每个单元与C1中相对应特征图的2*2邻域相连接。S2层每个单元的4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。每个单元的2*2感知机并不重叠，因此S2中每个特征图的大小是C1中特征图大小的1/4（行和列各1/2）。S2层有12（6*（1+1）=12）个可训练参数和5880（14*14*（2*2+1）*6=5880）个连接。</strong></li>
</ul>
<p><center><img src="http://i.imgur.com/Zzm048o.png" width="800"/></center></p>
<ul>
<li>
<p><strong>C3层也是一个卷积层，它同样通过5x5的卷积核去卷积层S2，然后得到的特征map就只有10x10个神经元，但是它有16种不同的卷积核，所以就存在16个特征map了。 C3中每个特征图由S2中所有6个或者几个特征map组合而成。为什么不把S2中的每个特征图连接到每个C3的特征图呢？原因有2点。第一，不完全的连接机制将连接的数量保持在合理的范围内。第二，也是最重要的，其破坏了网络的对称性。由于不同的特征图有不同的输入，所以迫使他们抽取不同的特征（希望是互补的）。</strong></p>
<p>例如，存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。这样C3层有1516（6*（3*25+1）+6*（4*25+1）+3*（4*25+1）+（25*6+1）=1516）个可训练参数和151600（10*10*1516=151600）个连接。</p>
</li>
</ul>
<p><center><img src="http://i.imgur.com/SiVPyWR.png" width="800"/></center></p>
<ul>
<li><strong>S4层是一个下采样层，由16个5*5大小的特征图构成。特征图中的每个单元与C3中相应特征图的2*2邻域相连接，跟C1和S2之间的连接一样。S4层有32个可训练参数（每个特征图1个因子和一个偏置16*（1+1）=32）和2000（16*（2*2+1）*5*5=2000）个连接。</strong></li>
</ul>
<p><center><img src="http://i.imgur.com/gTphBu6.png" width="800"/></center></p>
<ul>
<li><strong>C5层是一个卷积层，有120个特征图。每个单元与S4层的全部16个单元的5*5邻域相连。由于S4层特征图的大小也为5*5（同滤波器一样），故C5特征图的大小为1*1（5-5+1=1）：这构成了S4和C5之间的全连接。之所以仍将C5标示为卷积层而非全相联层，是因为如果LeNet-5的输入变大，而其他的保持不变，那么此时特征图的维数就会比1*1大。C5层有48120（120*（16*5*5+1）=48120由于与全部16个单元相连，故只加一个偏置）个可训练连接。</strong></li>
</ul>
<p><center><img src="http://i.imgur.com/6L3CmUc.png" width="800"/></center></p>
<ul>
<li><strong>F6层有84个单元（之所以选这个数字的原因来自于输出层的设计），与C5层全相连。有10164（84*(120*(1*1)+1)=10164）个可训练参数。如同经典神经网络，F6层计算输入向量和权重向量之间的点积，再加上一个偏置。然后将其传递给sigmoid函数产生单元i的一个状态。</strong></li>
</ul>
<p><center><img src="http://i.imgur.com/SNLgNWe.png" width="800"/></center>   </p>
<ul>
<li><strong>最后，输出层由欧式径向基函数（Euclidean Radial Basis Function）单元组成，每类一个单元，每个有84个输入。</strong></li>
</ul>
<p><center><img src="http://i.imgur.com/kJtbaEz.png" width="800"/></center></p>
</article>

        <footer>
            <nav>
                <ul>
                    <li><a href="http://www.shushilvshe.com/pages/about.html">About ME</a></li>
                    <li>:: <a href="http://www.shushilvshe.com/categories.html">Categories</a></li>
                    <li>:: <a href="http://www.shushilvshe.com/tags.html">Tags</a></li>
                </ul>
            </nav>
                <p id="theme-credit">Proudly powered by <a href="http://docs.notmyidea.org/alexis/pelican/">pelican</a></p>
        </footer>

</body>
</html>