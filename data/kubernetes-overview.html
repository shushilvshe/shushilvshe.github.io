<!DOCTYPE html>
<html lang="en">
<head>
        <title>shushilvshe's Blog : Kubernetes（1.8.3）系列之GPU配置</title>
        <meta charset="utf-8" />
        <link rel="stylesheet" href="http://www.shushilvshe.com/theme/css/main.css" type="text/css" />
        <link href="http://www.shushilvshe.com/" type="application/atom+xml" rel="alternate" title="shushilvshe's Blog ATOM Feed" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://www.shushilvshe.com/css/ie.css"/>
                <script src="http://www.shushilvshe.com/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://www.shushilvshe.com/css/ie6.css"/><![endif]-->

</head>

<body>
        
<header>
    <h1><a href="http://www.shushilvshe.com/data/kubernetes-overview.html" id="page-title">Kubernetes（1.8.3）系列之GPU配置</a></h1>
    <span id="sitename"><a href="http://www.shushilvshe.com" id="site-title">shushilvshe's Blog  <strong>Data Developer</strong></a> &sdot;</span>
<time datetime="2017-12-20T10:15:52+08:00">2017-12-20(星期三) 10:15</time></header>
<article>
    <p><link rel="stylesheet" href="http://yandex.st/highlightjs/6.2/styles/googlecode.min.css"></p>
<script src="http://code.jquery.com/jquery-1.7.2.min.js"></script>

<script src="http://yandex.st/highlightjs/6.2/highlight.min.js"></script>

<script>hljs.initHighlightingOnLoad();</script>

<script type="text/javascript">
 $(document).ready(function(){
      $("h2,h3,h4,h5,h6").each(function(i,item){
        var tag = $(item).get(0).localName;
        $(item).attr("id","wow"+i);
        $("#category").append('<a class="new'+tag+'" href="#wow'+i+'">'+$(this).text()+'</a></br>');
        $(".newh2").css("margin-left",0);
        $(".newh3").css("margin-left",20);
        $(".newh4").css("margin-left",40);
        $(".newh5").css("margin-left",60);
        $(".newh6").css("margin-left",80);
      });
 });
</script>

<div id="category"></div>

<p>Kubernetes支持容器请求GPU资源（目前仅支持NVIDIA GPU），在深度学习等场景中有大量应用。</p>
<h2>预先配置</h2>
<ul>
<li>Kubelet配置使用docker容器引擎（默认就是docker），其他容器引擎暂不支持该特性。</li>
<li>
<p>在所有的Node上安装Nvidia驱动，包括NVIDIA Cuda Toolkit和cuDNN等。</p>
<blockquote>
<p>PS:如果使用的是nvidia/cuda的docker镜像的话,则只需要安装Nvidia的驱动即可，另外安装一下对应docker版本的Nvidia-docker工具（Nvidia-docker更新速度相对docker慢一些）。可参考 <a href="http://www.shushilvshe.com/data/docker.html#wow33">Nvidia-docker的作用</a>， 在这里的作用简单来说就是提供了nvidia的一些命令和类库（cuda、）  </p>
</blockquote>
</li>
<li>
<p>在各台拥有GPU资源的节点上的kubelet上开启--feature-gates=Accelerators=true</p>
<div class="highlight"><pre><span></span><span class="k">[Unit]</span>
<span class="na">Description</span><span class="o">=</span><span class="s">Kubernetes Kubelet</span>
<span class="na">Documentation</span><span class="o">=</span><span class="s">https://github.com/GoogleCloudPlatform/kubernetes</span>
<span class="na">After</span><span class="o">=</span><span class="s">docker.service</span>
<span class="na">Requires</span><span class="o">=</span><span class="s">docker.service</span>

<span class="k">[Service]</span>
<span class="na">WorkingDirectory</span><span class="o">=</span><span class="s">/var/lib/kubelet</span>
<span class="na">ExecStart</span><span class="o">=</span><span class="s">/usr/local/bin/kubelet \</span>
<span class="s">  --feature-gates=Accelerators=true \</span>
<span class="s">  --cgroup-driver=cgroupfs \</span>
<span class="s">  --hostname-override=192.168.1.189 \</span>
<span class="s">  --pod-infra-container-image=192.168.1.184:5000/bigdata/rhel7/pod-infrastructure:v1.0.0 \</span>
<span class="s">  --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span>
<span class="s">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span>
<span class="s">  --cert-dir=/etc/kubernetes/ssl \</span>
<span class="s">  --cluster_dns=10.254.0.2 \</span>
<span class="s">  --cluster_domain=cluster.local. \</span>
<span class="s">  --hairpin-mode promiscuous-bridge \</span>
<span class="s">  --allow-privileged=true \</span>
<span class="s">  --fail-swap-on=false \</span>
<span class="s">  --serialize-image-pulls=false \</span>
<span class="s">  --logtostderr=true \</span>
<span class="s">  --max-pods=512 \</span>
<span class="s">  --v=2</span>

<span class="k">[Install]</span>
<span class="na">WantedBy</span><span class="o">=</span><span class="s">multi-user.target</span>
</pre></div>


</li>
</ul>
<h2>GPU 配置</h2>
<h3>同种型号的GPU</h3>
<p>使用 <code>alpha.kubernetes.io/nvidia-gpu</code> 资源进行GPU指定：</p>
<div class="highlight"><pre><span></span><span class="n">apiVersion</span><span class="o">:</span> <span class="n">v1</span>
<span class="n">kind</span><span class="o">:</span> <span class="n">Pod</span> 
<span class="n">metadata</span><span class="o">:</span>
  <span class="n">name</span><span class="o">:</span> <span class="n">gpu</span><span class="o">-</span><span class="n">pod</span>
<span class="n">spec</span><span class="o">:</span> 
  <span class="n">containers</span><span class="o">:</span> 
    <span class="o">-</span> <span class="n">name</span><span class="o">:</span> <span class="n">gpu</span><span class="o">-</span><span class="n">container</span><span class="o">-</span><span class="mi">1</span>
      <span class="n">image</span><span class="o">:</span> <span class="n">gcr</span><span class="o">.</span><span class="na">io</span><span class="sr">/google_containers/</span><span class="n">pause</span><span class="o">:</span><span class="mf">2.0</span>
      <span class="n">resources</span><span class="o">:</span> 
        <span class="n">limits</span><span class="o">:</span> 
          <span class="n">alpha</span><span class="o">.</span><span class="na">kubernetes</span><span class="o">.</span><span class="na">io</span><span class="o">/</span><span class="n">nvidia</span><span class="o">-</span><span class="n">gpu</span><span class="o">:</span> <span class="mi">2</span> <span class="err">#</span> <span class="n">requesting</span> <span class="mi">2</span> <span class="n">GPUs</span>
    <span class="o">-</span> <span class="n">name</span><span class="o">:</span> <span class="n">gpu</span><span class="o">-</span><span class="n">container</span><span class="o">-</span><span class="mi">2</span>
      <span class="n">image</span><span class="o">:</span> <span class="n">gcr</span><span class="o">.</span><span class="na">io</span><span class="sr">/google_containers/</span><span class="n">pause</span><span class="o">:</span><span class="mf">2.0</span>
      <span class="n">resources</span><span class="o">:</span> 
        <span class="n">limits</span><span class="o">:</span> 
          <span class="n">alpha</span><span class="o">.</span><span class="na">kubernetes</span><span class="o">.</span><span class="na">io</span><span class="o">/</span><span class="n">nvidia</span><span class="o">-</span><span class="n">gpu</span><span class="o">:</span> <span class="mi">3</span> <span class="err">#</span> <span class="n">requesting</span> <span class="mi">3</span> <span class="n">GPUs</span>
</pre></div>


<p>创建 Pod</p>
<div class="highlight"><pre><span></span>$ kubectl create -f pod.yaml
pod <span class="s2">&quot;gpu-pod&quot;</span> created
</pre></div>


<p><strong>注意:</strong></p>
<ul>
<li>GPU资源必须在resources.limits中请求，resources.requests中无效</li>
<li>容器可以请求1个或多个GPU，不能只请求一部分</li>
<li>多个容器之间不能共享GPU</li>
<li>默认假设所有Node安装了相同型号的GPU,不同型号的设置方式接下来讨论</li>
</ul>
<h3>多种型号的GPU</h3>
<p>如果集群Node中安装了多种型号的GPU，则可以使用Node Affinity来调度Pod到指定GPU型号的Node上。</p>
<p>首先，在集群初始化时，需要给Node打上GPU型号的标签：</p>
<div class="highlight"><pre><span></span><span class="x">NVIDIA_GPU_NAME=</span><span class="p">$(</span><span class="err">nvidia-smi</span> <span class="err">--query-gpu=gpu_name</span> <span class="err">--format=csv</span><span class="p">,</span><span class="err">noheader</span> <span class="err">--id=</span><span class="m">0</span> <span class="err">|</span> <span class="err">sed</span> <span class="err">-e</span> <span class="s1">&#39;s/ /-/g&#39;</span><span class="p">)</span><span class="x"></span>
<span class="x">source /etc/default/kubelet</span>
<span class="x">KUBELET_OPTS=&quot;</span><span class="p">$</span><span class="nv">KUBELET_OPTS</span><span class="x"> --node-labels=&#39;alpha.kubernetes.io/nvidia-gpu-name=</span><span class="p">$</span><span class="nv">NVIDIA_GPU_NAME</span><span class="x">&#39;&quot;</span>
<span class="x">echo &quot;KUBELET_OPTS=</span><span class="p">$</span><span class="nv">KUBELET_OPTS</span><span class="x">&quot; &gt; /etc/default/kubelet</span>
</pre></div>


<p>然后，在创建Pod时设置Node Affinity</p>
<div class="highlight"><pre><span></span><span class="n">kind</span><span class="o">:</span> <span class="n">pod</span>
<span class="n">apiVersion</span><span class="o">:</span> <span class="n">v1</span>
<span class="n">metadata</span><span class="o">:</span>
  <span class="n">annotations</span><span class="o">:</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="na">alpha</span><span class="o">.</span><span class="na">kubernetes</span><span class="o">.</span><span class="na">io</span><span class="o">/</span><span class="n">affinity</span><span class="o">:</span> <span class="o">&gt;</span>
      <span class="o">{</span>
        <span class="s2">&quot;nodeAffinity&quot;</span><span class="o">:</span> <span class="o">{</span>
          <span class="s2">&quot;requiredDuringSchedulingIgnoredDuringExecution&quot;</span><span class="o">:</span> <span class="o">{</span>
            <span class="s2">&quot;nodeSelectorTerms&quot;</span><span class="o">:</span> <span class="o">[</span>
              <span class="o">{</span>
                <span class="s2">&quot;matchExpressions&quot;</span><span class="o">:</span> <span class="o">[</span>
                  <span class="o">{</span>
                    <span class="s2">&quot;key&quot;</span><span class="o">:</span> <span class="s2">&quot;alpha.kubernetes.io/nvidia-gpu-name&quot;</span><span class="o">,</span>
                    <span class="s2">&quot;operator&quot;</span><span class="o">:</span> <span class="s2">&quot;In&quot;</span><span class="o">,</span>
                    <span class="s2">&quot;values&quot;</span><span class="o">:</span> <span class="o">[</span><span class="s2">&quot;Tesla K80&quot;</span><span class="o">,</span> <span class="s2">&quot;Tesla P100&quot;</span><span class="o">]</span>
                  <span class="o">}</span>
                <span class="o">]</span>
              <span class="o">}</span>
            <span class="o">]</span>
          <span class="o">}</span>
        <span class="o">}</span>
      <span class="o">}</span>
<span class="n">spec</span><span class="o">:</span>
  <span class="n">containers</span><span class="o">:</span>
    <span class="o">-</span>
      <span class="n">name</span><span class="o">:</span> <span class="n">gpu</span><span class="o">-</span><span class="n">container</span><span class="o">-</span><span class="mi">1</span>
      <span class="n">resources</span><span class="o">:</span>
        <span class="n">limits</span><span class="o">:</span>
          <span class="n">alpha</span><span class="o">.</span><span class="na">kubernetes</span><span class="o">.</span><span class="na">io</span><span class="o">/</span><span class="n">nvidia</span><span class="o">-</span><span class="n">gpu</span><span class="o">:</span> <span class="mi">2</span>
</pre></div>


<p>配置完后，该pod会使用 <code>Tesla K80</code> 和 <code>Tesla P100</code></p>
<h4>使用CUDA库</h4>
<p>可以通过hostPath volume的形式将CUDA库文件传给容器：</p>
<div class="highlight"><pre><span></span><span class="n">kind</span><span class="o">:</span> <span class="n">Pod</span>
<span class="n">apiVersion</span><span class="o">:</span> <span class="n">v1</span>
<span class="n">metadata</span><span class="o">:</span>
  <span class="n">name</span><span class="o">:</span> <span class="n">gpu</span><span class="o">-</span><span class="n">pod</span>
<span class="n">spec</span><span class="o">:</span>
  <span class="n">containers</span><span class="o">:</span>
  <span class="o">-</span> <span class="n">name</span><span class="o">:</span> <span class="n">gpu</span><span class="o">-</span><span class="n">container</span><span class="o">-</span><span class="mi">1</span>
    <span class="n">image</span><span class="o">:</span> <span class="n">gcr</span><span class="o">.</span><span class="na">io</span><span class="sr">/google_containers/</span><span class="n">pause</span><span class="o">:</span><span class="mf">2.0</span>
    <span class="n">resources</span><span class="o">:</span>
      <span class="n">limits</span><span class="o">:</span>
        <span class="n">alpha</span><span class="o">.</span><span class="na">kubernetes</span><span class="o">.</span><span class="na">io</span><span class="o">/</span><span class="n">nvidia</span><span class="o">-</span><span class="n">gpu</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">volumeMounts</span><span class="o">:</span>
    <span class="o">-</span> <span class="n">mountPath</span><span class="o">:</span> <span class="sr">/usr/local/nvidia/</span><span class="n">bin</span>
      <span class="n">name</span><span class="o">:</span> <span class="n">bin</span>
    <span class="o">-</span> <span class="n">mountPath</span><span class="o">:</span> <span class="sr">/usr/lib/</span><span class="n">nvidia</span>
      <span class="n">name</span><span class="o">:</span> <span class="n">lib</span>
  <span class="n">volumes</span><span class="o">:</span>
  <span class="o">-</span> <span class="n">hostPath</span><span class="o">:</span>
      <span class="n">path</span><span class="o">:</span> <span class="sr">/usr/lib/nvidia-375/</span><span class="n">bin</span>
    <span class="n">name</span><span class="o">:</span> <span class="n">bin</span>
  <span class="o">-</span> <span class="n">hostPath</span><span class="o">:</span>
      <span class="n">path</span><span class="o">:</span> <span class="sr">/usr/lib/</span><span class="n">nvidia</span><span class="o">-</span><span class="mi">375</span>
    <span class="n">name</span><span class="o">:</span> <span class="n">lib</span>
</pre></div>


<h3>kubernetes（1.8.3版本）自动分配GPU资源策略测试：</h3>
<p><strong>官网GPU说明：</strong></p>
<ul>
<li>Containers (and pods) do not share GPUs.</li>
<li>Each container can request one or more GPUs.</li>
<li>It is not possible to request a portion of a GPU.</li>
<li>Nodes are expected to be homogenous, i.e. run the same GPU hardware.</li>
</ul>
<p><strong>测试环境：</strong></p>
<blockquote>
<p>六个节点的集群，集群管理节点为单独一台机器，没有GPU，余下五个节点，有一个节点有四个GPU，其余四台机器均为各有一个GPU。GPU均为NVIDIA GeForce 1080 Ti。</p>
</blockquote>
<p><strong>结果：</strong></p>
<ul>
<li>1.每个任务申请的GPU只能是在一台节点上；</li>
<li>2.每个任务申请的GPU个数不能超过单台节点拥有的GPU个数；</li>
<li>3.kubernetes会自动分配符合GPU申请个数的节点和GPU；</li>
<li>4.可以指定在某台节点上分配GPU资源，但除了第一块GPU，其余GPU不可以自己指定；</li>
</ul>
<p>------------------------------2017-12-21 补充</p>
<ul>
<li>
<p>5.当指定特权模式之后，任意GPU都可以指定了（可能还有别的方法可以实现）。</p>
<p>在yaml文件中的containers项下配置打开特权模式：</p>
<div class="highlight"><pre><span></span>containers: 
- name: tensorflow-py2 
  image: 192.168.1.184:5000/bigdata/tensorflow-gpu-py2:1.3.0-gpu
  securityContext:
    privileged: true
  workingDir: /ceph/docker/
</pre></div>


</li>
</ul>
</article>

        <footer>
            <nav>
                <ul>
                    <li><a href="http://www.shushilvshe.com/pages/about.html">About ME</a></li>
                    <li>:: <a href="http://www.shushilvshe.com/categories.html">Categories</a></li>
                    <li>:: <a href="http://www.shushilvshe.com/tags.html">Tags</a></li>
                </ul>
            </nav>
                <p id="theme-credit">Proudly powered by <a href="http://docs.notmyidea.org/alexis/pelican/">pelican</a></p>
        </footer>

</body>
</html>